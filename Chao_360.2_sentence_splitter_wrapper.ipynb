{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b742f3d2995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "import stanfordnlp, time, csv, re, os\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\"Base tokenizer/tokens classes and utilities.\"\"\"\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "class Tokens(object):\n",
    "    \"\"\"A class to represent a list of tokenized text.\"\"\"\n",
    "    TEXT = 0\n",
    "    TEXT_WS = 1\n",
    "    SPAN = 2\n",
    "    POS = 3\n",
    "    LEMMA = 4\n",
    "    NER = 5\n",
    "\n",
    "    def __init__(self, data, annotators, opts=None, output = None):\n",
    "        self.data = data\n",
    "        self.annotators = annotators\n",
    "        self.opts = opts or {}\n",
    "        if output != None:\n",
    "            self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def slice(self, i=None, j=None):\n",
    "        \"\"\"Return a view of the list of tokens from [i, j).\"\"\"\n",
    "        new_tokens = copy.copy(self)\n",
    "        new_tokens.data = self.data[i: j]\n",
    "        return new_tokens\n",
    "\n",
    "    def ssplit(self):\n",
    "        s_list = []\n",
    "        original_sentence = self.untokenize()\n",
    "        dict_a = self.output\n",
    "        for i in dict_a['sentences']:\n",
    "            start_offset = i['tokens'][0]['characterOffsetBegin']\n",
    "            end_offset = i['tokens'][-1]['characterOffsetEnd']\n",
    "            s_list.append(original_sentence[start_offset:end_offset+1].strip())\n",
    "\n",
    "        return s_list\n",
    "\n",
    "    def untokenize(self):\n",
    "        \"\"\"Returns the original text (with whitespace reinserted).\"\"\"\n",
    "        return ''.join([t[self.TEXT_WS] for t in self.data]).strip()\n",
    "\n",
    "    def words(self, uncased=False):\n",
    "        \"\"\"Returns a list of the text of each token\n",
    "\n",
    "        Args:\n",
    "            uncased: lower cases text\n",
    "        \"\"\"\n",
    "        if uncased:\n",
    "            return [t[self.TEXT].lower() for t in self.data]\n",
    "        else:\n",
    "            return [t[self.TEXT] for t in self.data]\n",
    "\n",
    "    def offsets(self):\n",
    "        \"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"\n",
    "        return [t[self.SPAN] for t in self.data]\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"Returns a list of part-of-speech tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'pos' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.POS] for t in self.data]\n",
    "\n",
    "    def lemmas(self):\n",
    "        \"\"\"Returns a list of the lemmatized text of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'lemma' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.LEMMA] for t in self.data]\n",
    "\n",
    "    def entities(self):\n",
    "        \"\"\"Returns a list of named-entity-recognition tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'ner' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.NER] for t in self.data]\n",
    "\n",
    "    def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "        \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "\n",
    "        Args:\n",
    "            n: upper limit of ngram length\n",
    "            uncased: lower cases text\n",
    "            filter_fn: user function that takes in an ngram list and returns\n",
    "              True or False to keep or not keep the ngram\n",
    "            as_string: return the ngram as a string vs list\n",
    "        \"\"\"\n",
    "        def _skip(gram):\n",
    "            if not filter_fn:\n",
    "                return False\n",
    "            return filter_fn(gram)\n",
    "\n",
    "        words = self.words(uncased)\n",
    "        ngrams = [(s, e + 1)\n",
    "                  for s in range(len(words))\n",
    "                  for e in range(s, min(s + n, len(words)))\n",
    "                  if not _skip(words[s:e + 1])]\n",
    "\n",
    "        # Concatenate into strings\n",
    "        if as_strings:\n",
    "            ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def entity_groups(self):\n",
    "        \"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"\n",
    "        entities = self.entities()\n",
    "        if not entities:\n",
    "            return None\n",
    "        non_ent = self.opts.get('non_ent', 'O')\n",
    "        groups = []\n",
    "        idx = 0\n",
    "        while idx < len(entities):\n",
    "            ner_tag = entities[idx]\n",
    "            # Check for entity tag\n",
    "            if ner_tag != non_ent:\n",
    "                # Chomp the sequence\n",
    "                start = idx\n",
    "                while (idx < len(entities) and entities[idx] == ner_tag):\n",
    "                    idx += 1\n",
    "                groups.append((self.slice(start, idx).untokenize(), ner_tag))\n",
    "            else:\n",
    "                idx += 1\n",
    "        return groups\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \"\"\"Base tokenizer class.\n",
    "    Tokenizers implement tokenize, which should return a Tokens class.\n",
    "    \"\"\"\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        self.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-308cd02011a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCoreNLPTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\"Simple wrapper around the Stanford CoreNLP pipeline.\n",
    "\n",
    "Serves commands to a java subprocess running the jar. Requires java 8.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import pexpect\n",
    "\n",
    "# from .tokenizer import Tokens, Tokenizer\n",
    "# from . import DEFAULTS\n",
    "\n",
    "\n",
    "class CoreNLPTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotators: set that can include pos, lemma, and ner.\n",
    "            classpath: Path to the corenlp directory of jars\n",
    "            mem: Java heap memory\n",
    "        \"\"\"\n",
    "        self.classpath = ('/home/chao/stanfordnlp/demo/corenlp/*')\n",
    "        self.annotators = copy.deepcopy(kwargs.get('annotators', set()))\n",
    "        self.mem = kwargs.get('mem', '2g')\n",
    "        self._launch()\n",
    "\n",
    "    def _launch(self):\n",
    "        \"\"\"Start the CoreNLP jar with pexpect.\"\"\"\n",
    "        annotators = ['tokenize', 'ssplit']\n",
    "        if 'ner' in self.annotators:\n",
    "            annotators.extend(['pos', 'lemma', 'ner'])\n",
    "        elif 'lemma' in self.annotators:\n",
    "            annotators.extend(['pos', 'lemma'])\n",
    "        elif 'pos' in self.annotators:\n",
    "            annotators.extend(['pos'])\n",
    "        annotators = ','.join(annotators)\n",
    "        options = ','.join(['untokenizable=noneDelete',\n",
    "                            'invertible=true'])\n",
    "        cmd = ['java', '-mx' + self.mem, '-cp', '\"%s\"' % self.classpath,\n",
    "               'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators',\n",
    "               annotators, '-tokenize.options', options,\n",
    "               '-outputFormat', 'json', '-prettyPrint', 'false']\n",
    "\n",
    "        # We use pexpect to keep the subprocess alive and feed it commands.\n",
    "        # Because we don't want to get hit by the max terminal buffer size,\n",
    "        # we turn off canonical input processing to have unlimited bytes.\n",
    "        self.corenlp = pexpect.spawn('/bin/bash', maxread=100000, timeout=60)\n",
    "        self.corenlp.setecho(False)\n",
    "        self.corenlp.sendline('stty -icanon')\n",
    "        self.corenlp.sendline(' '.join(cmd))\n",
    "        self.corenlp.delaybeforesend = 0\n",
    "        self.corenlp.delayafterread = 0\n",
    "        self.corenlp.expect_exact('NLP>', searchwindowsize=100)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert(token):\n",
    "        if token == '-LRB-':\n",
    "            return '('\n",
    "        if token == '-RRB-':\n",
    "            return ')'\n",
    "        if token == '-LSB-':\n",
    "            return '['\n",
    "        if token == '-RSB-':\n",
    "            return ']'\n",
    "        if token == '-LCB-':\n",
    "            return '{'\n",
    "        if token == '-RCB-':\n",
    "            return '}'\n",
    "        return token\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Since we're feeding text to the commandline, we're waiting on seeing\n",
    "        # the NLP> prompt. Hacky!\n",
    "        if 'NLP>' in text:\n",
    "            raise RuntimeError('Bad token (NLP>) in text!')\n",
    "\n",
    "        # Sending q will cause the process to quit -- manually override\n",
    "        if text.lower().strip() == 'q':\n",
    "            token = text.strip()\n",
    "            index = text.index(token)\n",
    "            data = [(token, text[index:], (index, index + 1), 'NN', 'q', 'O')]\n",
    "            return Tokens(data, self.annotators)\n",
    "\n",
    "        # Minor cleanup before tokenizing.\n",
    "        clean_text = text.replace('\\n', ' ')\n",
    "\n",
    "        self.corenlp.sendline(clean_text.encode('utf-8'))\n",
    "        self.corenlp.expect_exact('NLP>', searchwindowsize=100)\n",
    "\n",
    "        # Skip to start of output (may have been stderr logging messages)\n",
    "        output = self.corenlp.before\n",
    "        start = output.find(b'{\\r\\n  \"sentences\":')\n",
    "        output = json.loads(output[start:].decode('utf-8'))\n",
    "\n",
    "        data = []\n",
    "        tokens = [t for s in output['sentences'] for t in s['tokens']]\n",
    "#         print(output)\n",
    "        for i in range(len(tokens)):\n",
    "            # Get whitespace\n",
    "            start_ws = tokens[i]['characterOffsetBegin']\n",
    "            if i + 1 < len(tokens):\n",
    "                end_ws = tokens[i + 1]['characterOffsetBegin']\n",
    "            else:\n",
    "                end_ws = tokens[i]['characterOffsetEnd']\n",
    "\n",
    "            data.append((\n",
    "                self._convert(tokens[i]['word']),\n",
    "                text[start_ws: end_ws],\n",
    "                (tokens[i]['characterOffsetBegin'],\n",
    "                 tokens[i]['characterOffsetEnd']),\n",
    "                tokens[i].get('pos', None),\n",
    "                tokens[i].get('lemma', None),\n",
    "                tokens[i].get('ner', None)\n",
    "            ))\n",
    "        return Tokens(data, self.annotators, output = output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CoreNLPTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6db08ec47f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CoreNLPTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tok = CoreNLPTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_sentence_splitter(text):\n",
    "    \n",
    "    sent_list = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sent_list.append(sent.text)\n",
    "        \n",
    "    return sent_list\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = [token.text for token in doc]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = \"/home/chao/research_1_newsela_alignment_898/newsela_article_corpus_2016-01-29/articles/\"\n",
    "newsela_data_path = \"/home/chao/research_1_newsela_alignment_898/data/current_2019_10_11_v12.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file_newsela(path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='|', quotechar='', quoting=csv.QUOTE_NONE)    \n",
    "        for idx, line in enumerate(reader):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "                data.append(line)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_article_names(path):    \n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='|', quotechar='', quoting=csv.QUOTE_NONE)    \n",
    "        name = []\n",
    "        for idx, line in enumerate(reader):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                tmp = [i for i,val in enumerate(line[0]) if val==\"-\"]\n",
    "                sent_article_name = line[0][tmp[0]+1:tmp[-6]]\n",
    "\n",
    "                name.append(sent_article_name)\n",
    "\n",
    "    name = list(set(name))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article(path):\n",
    "    with open(path) as f:\n",
    "        data = f.readlines()\n",
    "        data = [i.strip() for i in data]\n",
    "        data = [i for i in data if i != \"\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = extract_all_article_names(newsela_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_article = {}\n",
    "\n",
    "for i in all_names:\n",
    "    for j in range(5):\n",
    "        path = \"{}{}.{}.txt\".format(article_path, i, j)\n",
    "        article = read_article(path)\n",
    "        path_to_article[path] = article\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ac48c6434d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi. I am Chao. I come from No.1 Middle School (XX School).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tok' is not defined"
     ]
    }
   ],
   "source": [
    "tok.tokenize(\"Hi. I am Chao. I come from No.1 Middle School (XX School).\").ssplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714af8a6c628442d8df1ec6a9d67d281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "error_cases = []\n",
    "\n",
    "for k, v in tqdm(path_to_article.items()):\n",
    "\n",
    "    for line in v:\n",
    "        if line.startswith(\"##\"):\n",
    "            continue\n",
    "        else:\n",
    "            line = \" \".join(line.split()) # this step is quite important, in the original sent, there is \\xa0 looks likea white space\n",
    "            ssplit = tok.tokenize(line).ssplit()\n",
    "            ssplit_join = \" \".join(ssplit)\n",
    "            if ssplit_join != line:\n",
    "                \n",
    "                if len(ssplit_join) == len(line):\n",
    "                    for char_idx, char in enumerate(ssplit_join):\n",
    "                        if ssplit_join[char_idx] != line[char_idx]:\n",
    "                            print(\"D-{}-{}-{}-{}-\".format(len(line), char_idx, \\\n",
    "                                                          ssplit_join[char_idx].encode('unicode_escape').decode(), \\\n",
    "                                                          line[char_idx].encode('unicode_escape').decode()) )\n",
    "                \n",
    "                    count += 1\n",
    "                    print(line)\n",
    "                    print(\" \".join(ssplit))\n",
    "                    print(\"\\n\")\n",
    "                    for i in ssplit:\n",
    "                        print(i)\n",
    "                    print(\"\\n\\n\")\n",
    "                    \n",
    "                if len(ssplit_join) != len(line):\n",
    "                    count += 1\n",
    "                    print(line)\n",
    "                    print(\" \".join(ssplit))\n",
    "                    print(\"\\n\")\n",
    "                    for i in ssplit:\n",
    "                        print(i)\n",
    "                    print(\"\\n\\n\")\n",
    "                \n",
    "                \n",
    "print(count)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
